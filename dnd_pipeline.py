"""
title: Custom RAG Pipeline
author: your_name
date: 2025-06-13
version: 1.0
license: MIT
description: A custom RAG pipeline using ChromaDB and Qwen for generating structured markdown answers.
requirements: chromadb, openai, pypdf, python-docx, langchain
"""

from typing import List, Union, Generator, Iterator, Optional
from pydantic import BaseModel
import chromadb
from openai import OpenAI

# åµŒå…¥å‡½æ•°å°è£…
class OpenAIEmbeddingFunction:
    def __init__(self, client):
        self.client = client

    def __call__(self, input):
        resp = self.client.embeddings.create(
            input=input,
            model="text-embedding-v3"
        )
        return [item.embedding for item in resp.data]

    def name(self):
        return "text-embedding-v3"

# RAG æ ¸å¿ƒ
class ReportGenerator:
    def __init__(self):
        self.openai_client = OpenAI(
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            api_key="sk-442562cd6b6b4b2896ebdac8ce8d047e"  # æ›¿æ¢ä¸ºä½ çš„ key
        )
        self.embed_fn = OpenAIEmbeddingFunction(self.openai_client)
        self.client = chromadb.PersistentClient(path="db")
        self.collection = self.client.get_collection(
            "linux_course",
            embedding_function=self.embed_fn
        )

    def generate_report(self, topic: str):
        print("ğŸ” æ­£åœ¨è¿›è¡ŒåµŒå…¥")
        query_vector = self.embed_fn([topic])
        print("âœ… åµŒå…¥å®Œæˆ")

        results = self.collection.query(query_embeddings=query_vector, n_results=5)
        context = "\n\n".join(results['documents'][0])

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„DnD 5e ä¸»æŒäººï¼Œä½ éœ€è¦æ ¹æ®ä»¥ä¸‹çŸ¥è¯†åº“ä¸­çš„å†…å®¹å¯¹ç©å®¶çš„æé—®æä¾›ä¸°å¯Œï¼Œæ°å½“çš„çŸ¥è¯†å’Œä»‹ç»

## çŸ¥è¯†åº“ç‰‡æ®µ ##
{context}

## æŠ¥å‘Šè¦æ±‚ ##
- ä¿æŒä¸“ä¸šä½†å‹å¥½çš„è¯­è°ƒã€‚å¦‚æœç©å®¶è¯¢é—®è§„åˆ™ï¼Œè¯·æä¾›å‡†ç¡®çš„D&D 5Eè§„åˆ™ä¿¡æ¯ã€‚
- å¯ä½¿ç”¨markdownæ ¼å¼è¿›è¡Œå›å¤
"""

        print("ğŸ¤– æ­£åœ¨è°ƒç”¨ qwen")
        response = self.openai_client.chat.completions.create(
            model='qwen-plus',
            messages=[{'role': 'user', 'content': prompt}]
        )
        ans = response.choices[0].message.content
        print("âœ… qwen å“åº”å®Œæˆ")
        return ans

# Pipeline ä¸»ä½“
class Pipeline:
    def __init__(self):
        self.generator = ReportGenerator()

    async def on_startup(self):
        print("âœ… RAG Pipeline å¯åŠ¨")

    async def on_shutdown(self):
        print("ğŸ›‘ RAG Pipeline å…³é—­")

    async def inlet(self, body: dict, user: Optional[dict] = None) -> dict:
        return body  # å¯åŠ å…¥é¢„å¤„ç†é€»è¾‘

    async def outlet(self, body: dict, user: Optional[dict] = None) -> dict:
        return body  # å¯åŠ å…¥åå¤„ç†é€»è¾‘

    def pipe(
        self,
        user_message: str,
        model_id: str,
        messages: List[dict],
        body: dict,
    ) -> Union[str, Generator, Iterator]:
        print(f"ğŸ“© æ”¶åˆ°è¯·æ±‚: {user_message}")
        return self.generator.generate_report(user_message)
